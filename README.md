ğŸ–ï¸ Real-Time Sign Language Translator

A real-time American Sign Language (ASL) translation system that detects hand gestures using computer vision and translates them into readable text and spoken audio.

ğŸ“Œ Overview

This project uses computer vision and deep learning to recognize sign language gestures from a live camera feed and convert them into text and speech. The goal is to improve accessibility and communication for individuals who rely on sign language.

The system performs:

Real-time hand detection

Gesture classification using a Convolutional Neural Network (CNN)

Text output generation

Optional text-to-speech audio playback

ğŸš€ Features

ğŸ“· Real-time webcam gesture detection

ğŸ§  Custom CNN model for sign classification

ğŸ”¤ Text output of recognized signs

ğŸ”Š Text-to-speech conversion

ğŸ“Š Model evaluation with accuracy metrics

ğŸ§¹ Preprocessing pipeline (resizing, normalization, landmark extraction)

ğŸ› ï¸ Tech Stack

Python

OpenCV â€“ video capture & image processing

MediaPipe â€“ hand landmark detection (if used)

TensorFlow / PyTorch â€“ CNN model training

NumPy / Pandas â€“ data processing

Matplotlib â€“ visualization

pyttsx3 / gTTS â€“ text-to-speech

ğŸ§  Model Architecture

The gesture classification model is a Convolutional Neural Network consisting of:

Convolutional layers (feature extraction)

ReLU activation functions

Max pooling layers

Fully connected layers

Softmax output layer

Loss Function: Cross-Entropy
Optimizer: Adam
Evaluation Metrics: Accuracy, Confusion Matrix
